TrainingArguments:
  warmup_steps: 10
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  max_steps: 10
  learning_rate: 2e-4
  logging_steps: 1
  output_dir: "outputs"
  optim: "paged_adamw_8bit"
  save_strategy: "steps"

LoraConfig:
  r: 64
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

BitsandBytesConfig:
  load_in_4bit: True
  bnb_4bit_use_double_quant: True
  bnb_4bit_quant_type: "nf4"