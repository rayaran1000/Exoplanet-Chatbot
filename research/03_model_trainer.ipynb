{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects for portfolio\\\\Exoplanet Chatbot\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects for portfolio\\\\Exoplanet Chatbot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig: # defined for the config components present in artifacts for model training\n",
    "    root_dir : Path \n",
    "    data_path : Path\n",
    "    tokenizer_ckpt : Path\n",
    "    model_ckpt : Path\n",
    "    model_save_path: Path\n",
    "    tokenizer_save_path: Path\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    max_steps: int \n",
    "    learning_rate: float\n",
    "    logging_steps: int\n",
    "    output_dir: str\n",
    "    optim: str\n",
    "    save_strategy: str\n",
    "    r: int\n",
    "    lora_alpha: int\n",
    "    lora_dropout: int\n",
    "    bias: int\n",
    "    task_type: int\n",
    "    load_in_4bit: bool\n",
    "    bnb_4bit_use_double_quant: bool\n",
    "    bnb_4bit_quant_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from exoplanet_chatbot.constants import *\n",
    "from exoplanet_chatbot.utils.common import read_yaml,create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH):\n",
    "    # Here we are reading the yaml file and we can now use the file paths and parameter values present inside pararms and config.yaml        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) # Here we are calling the artifacts_root key values using '.' , which was the purpose of @ensure_annotations\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "\n",
    "        config= self.config.model_trainer # Calling the model_trainer dictionary created in config.yaml file\n",
    "        params_training=self.params.TrainingArguments # Calling the TrainingArguments dictionary in params.yaml file\n",
    "        params_lora = self.params.LoraConfig # Calling the Lora Config dictionary in params.yaml file\n",
    "        params_bnb = self.params.BitsandBytesConfig # Calling the BitsandBytesConfig dictionary in params.yaml file\n",
    "\n",
    "        create_directories([config.root_dir]) # Creating a directory using the root directory\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig( # Extracting the values from the config.yaml to here inside data_ingestion_config\n",
    "            #Config parameters\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_ckpt=config.tokenizer_ckpt,\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            model_save_path=config.model_save_path,\n",
    "            tokenizer_save_path=config.tokenizer_save_path,\n",
    "\n",
    "            #Training parameters\n",
    "            warmup_steps=params_training.warmup_steps,\n",
    "            per_device_train_batch_size=params_training.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=params_training.gradient_accumulation_steps,\n",
    "            max_steps=params_training.max_steps,\n",
    "            learning_rate=params_training.learning_rate,\n",
    "            logging_steps=params_training.logging_steps,\n",
    "            output_dir=params_training.output_dir,\n",
    "            optim=params_training.optim,\n",
    "            save_strategy=params_training.save_strategy,\n",
    "\n",
    "            #Lora parameters\n",
    "            r=params_lora.r,\n",
    "            lora_alpha=params_lora.lora_alpha,\n",
    "            lora_dropout=params_lora.lora_dropout,\n",
    "            bias=params_lora.bias,\n",
    "            task_type=params_lora.task_type,\n",
    "\n",
    "            #Bits and bytes Configuration\n",
    "            load_in_4bit=params_bnb.load_in_4bit,\n",
    "            bnb_4bit_use_double_quant=params_bnb.bnb_4bit_use_double_quant,\n",
    "            bnb_4bit_quant_type=params_bnb.bnb_4bit_quant_type\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-16 23:40:48,218: INFO: config: PyTorch version 2.2.2+cu121 available.]\n",
      "[2024-06-16 23:40:48,220: INFO: config: TensorFlow version 2.16.1 available.]\n"
     ]
    }
   ],
   "source": [
    "# Model Trainer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "from transformers import TrainingArguments, TrainerCallback, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "class DebugCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Step: {state.global_step}, Loss: {state.log_history[-1]['loss'] if state.log_history else 'N/A'}\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self,config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.bnb_config = BitsAndBytesConfig(load_in_4bit=self.config.load_in_4bit, bnb_4bit_use_double_quant=self.config.bnb_4bit_use_double_quant, \n",
    "                                             bnb_4bit_quant_type=self.config.bnb_4bit_quant_type, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.config.model_ckpt, quantization_config=self.bnb_config, device_map={\"\":0})\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_ckpt)\n",
    "    \n",
    "    # Dataset Train Test Split\n",
    "    def train_test_split(self,data):\n",
    "\n",
    "        data = data.train_test_split(test_size=0.2)\n",
    "        train_data = data[\"train\"]\n",
    "        test_data = data[\"test\"]\n",
    "\n",
    "        return (train_data,test_data)\n",
    "\n",
    "    # Dataset Creating and Tokenization for Finetuning\n",
    "    def transform_and_tokenize(self):\n",
    "\n",
    "        # Loading the finetuning dataset\n",
    "        finetune_dataframe = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        # Converting the dataframe to dataset\n",
    "        finetune_dataset = Dataset.from_pandas(finetune_dataframe)\n",
    "\n",
    "        # Shuffling and Tokenization\n",
    "        finetune_dataset = finetune_dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "        finetune_dataset = finetune_dataset.map(lambda samples: self.tokenizer(samples[\"prompt\"]), batched=True)\n",
    "\n",
    "        # Train Test Split of Dataset\n",
    "        train_dataset,test_dataset = self.train_test_split(finetune_dataset)\n",
    "\n",
    "        return (train_dataset,test_dataset)\n",
    "    \n",
    "    # Function for preparing the linear layers for training in LoRa\n",
    "    def find_all_linear_names(self,model):\n",
    "        cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "        lora_module_names = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                names = name.split('.')\n",
    "                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "            if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "                lora_module_names.remove('lm_head')\n",
    "        return list(lora_module_names)\n",
    "\n",
    "    # LoRa Configuration for training\n",
    "    def Lora_config(self):\n",
    "\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "        modules = self.find_all_linear_names(self.model)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.r, #  Always keep it 2 times the lora_alpha\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "            target_modules=modules,\n",
    "            lora_dropout=self.config.lora_dropout,\n",
    "            bias=self.config.bias,\n",
    "            task_type=self.config.task_type\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "\n",
    "        return (self.model,lora_config)\n",
    "\n",
    "    def Model_Config(self):\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            warmup_steps = self.config.warmup_steps,\n",
    "            per_device_train_batch_size = self.config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = self.config.gradient_accumulation_steps,\n",
    "            max_steps = self.config.max_steps,\n",
    "            learning_rate = float(self.config.learning_rate),\n",
    "            logging_steps = self.config.logging_steps,\n",
    "            output_dir = self.config.output_dir,\n",
    "            optim = self.config.optim,\n",
    "            save_strategy = self.config.save_strategy\n",
    "        )\n",
    "\n",
    "        return training_args\n",
    "    \n",
    "    def tokenizer_and_model_save(self,model):\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_ckpt,\n",
    "            low_cpu_mem_usage=True,\n",
    "            return_dict=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={\"\": 0},\n",
    "        )\n",
    "        merged_model= PeftModel.from_pretrained(base_model, model)\n",
    "        merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "        # Save the merged model\n",
    "        merged_model.save_pretrained(self.config.model_save_path,safe_serialization=True)\n",
    "        self.tokenizer.save_pretrained(self.config.tokenizer_save_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Loading the tokenized datasets\n",
    "        train_dataset_tokenized , validation_dataset_tokenized = self.transform_and_tokenize()\n",
    "\n",
    "        # Loading the LoRa configured model\n",
    "        model,lora_config = self.Lora_config()\n",
    "\n",
    "        # Setting the Trainer       \n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset_tokenized,\n",
    "            eval_dataset=validation_dataset_tokenized,\n",
    "            dataset_text_field=\"prompt\",\n",
    "            peft_config=lora_config,\n",
    "            args=self.Model_Config(),\n",
    "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),callbacks=[DebugCallback()]\n",
    "        )\n",
    "\n",
    "        # Training the model\n",
    "        model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "        trainer.train()\n",
    "\n",
    "        #Saving the tokenizer and Model\n",
    "        new_model = \"gemma-Exochat-Instruct-Finetune-Step10\"\n",
    "        trainer.model.save_pretrained(new_model)\n",
    "        self.tokenizer_and_model_save(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-16 23:40:48,782: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-06-16 23:40:48,788: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-06-16 23:40:48,789: INFO: common: created directory at: artifacts]\n",
      "[2024-06-16 23:40:48,791: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b59ae293aa94e2eb5fc30d6f7dd3fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49017043481423099e9ed9c7f5fb385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c374806dc994fcfbb6a8a50ed7974fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d83270524b47928a55f068be532bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5830b66770a4614836ba23b8e1eea7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:555: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: N/A\n",
      "{'loss': 3.386, 'grad_norm': 2.501392364501953, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "Step: 2, Loss: 3.386\n",
      "{'loss': 3.5357, 'grad_norm': 2.5799057483673096, 'learning_rate': 0.00017777777777777779, 'epoch': 0.0}\n",
      "Step: 3, Loss: 3.5357\n",
      "{'loss': 2.5002, 'grad_norm': 1.47601318359375, 'learning_rate': 0.00015555555555555556, 'epoch': 0.0}\n",
      "Step: 4, Loss: 2.5002\n",
      "{'loss': 2.0814, 'grad_norm': 1.46877121925354, 'learning_rate': 0.00013333333333333334, 'epoch': 0.0}\n",
      "Step: 5, Loss: 2.0814\n",
      "{'loss': 1.7645, 'grad_norm': 1.7003042697906494, 'learning_rate': 0.00011111111111111112, 'epoch': 0.0}\n",
      "Step: 6, Loss: 1.7645\n",
      "{'loss': 1.5005, 'grad_norm': 1.3736467361450195, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.0}\n",
      "Step: 7, Loss: 1.5005\n",
      "{'loss': 1.3533, 'grad_norm': 1.3190935850143433, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.0}\n",
      "Step: 8, Loss: 1.3533\n",
      "{'loss': 1.1126, 'grad_norm': 1.1966962814331055, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.0}\n",
      "Step: 9, Loss: 1.1126\n",
      "{'loss': 1.0749, 'grad_norm': 1.1313321590423584, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.0}\n",
      "Step: 10, Loss: 1.0749\n",
      "{'loss': 1.0258, 'grad_norm': 1.1701029539108276, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 769.4553, 'train_samples_per_second': 0.052, 'train_steps_per_second': 0.013, 'train_loss': 1.9334859371185302, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765f03ee763744d4b9bb435a3cc4719c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config() # Storing the configuration\n",
    "    model_training = ModelTrainer(config=model_trainer_config) # Using the configuration saved earlier to call model_training\n",
    "    model_training.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
