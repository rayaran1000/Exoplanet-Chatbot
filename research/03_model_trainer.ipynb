{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects for portfolio\\\\Exoplanet Chatbot\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects for portfolio\\\\Exoplanet Chatbot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig: # defined for the config components present in artifacts for model training\n",
    "    root_dir : Path \n",
    "    data_path : Path\n",
    "    tokenizer_ckpt : Path\n",
    "    model_ckpt : Path\n",
    "    model_save_path: Path\n",
    "    tokenizer_save_path: Path\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    max_steps: int \n",
    "    learning_rate: float\n",
    "    logging_steps: int\n",
    "    output_dir: str\n",
    "    optim: str\n",
    "    save_strategy: str\n",
    "    r: int\n",
    "    lora_alpha: int\n",
    "    lora_dropout: int\n",
    "    bias: int\n",
    "    task_type: int\n",
    "    load_in_4bit: bool\n",
    "    bnb_4bit_use_double_quant: bool\n",
    "    bnb_4bit_quant_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from exoplanet_chatbot.constants import *\n",
    "from exoplanet_chatbot.utils.common import read_yaml,create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH):\n",
    "    # Here we are reading the yaml file and we can now use the file paths and parameter values present inside pararms and config.yaml        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) # Here we are calling the artifacts_root key values using '.' , which was the purpose of @ensure_annotations\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "\n",
    "        config= self.config.model_trainer # Calling the model_trainer dictionary created in config.yaml file\n",
    "        params_training=self.params.TrainingArguments # Calling the TrainingArguments dictionary in params.yaml file\n",
    "        params_lora = self.params.LoraConfig # Calling the Lora Config dictionary in params.yaml file\n",
    "        params_bnb = self.params.BitsandBytesConfig # Calling the BitsandBytesConfig dictionary in params.yaml file\n",
    "\n",
    "        create_directories([config.root_dir]) # Creating a directory using the root directory\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig( # Extracting the values from the config.yaml to here inside data_ingestion_config\n",
    "            #Config parameters\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_ckpt=config.tokenizer_ckpt,\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            model_save_path=config.model_save_path,\n",
    "            tokenizer_save_path=config.tokenizer_save_path,\n",
    "\n",
    "            #Training parameters\n",
    "            warmup_steps=params_training.warmup_steps,\n",
    "            per_device_train_batch_size=params_training.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=params_training.gradient_accumulation_steps,\n",
    "            max_steps=params_training.max_steps,\n",
    "            learning_rate=params_training.learning_rate,\n",
    "            logging_steps=params_training.logging_steps,\n",
    "            output_dir=params_training.output_dir,\n",
    "            optim=params_training.optim,\n",
    "            save_strategy=params_training.save_strategy,\n",
    "\n",
    "            #Lora parameters\n",
    "            r=params_lora.r,\n",
    "            lora_alpha=params_lora.lora_alpha,\n",
    "            lora_dropout=params_lora.lora_dropout,\n",
    "            bias=params_lora.bias,\n",
    "            task_type=params_lora.task_type,\n",
    "\n",
    "            #Bits and bytes Configuration\n",
    "            load_in_4bit=params_bnb.load_in_4bit,\n",
    "            bnb_4bit_use_double_quant=params_bnb.bnb_4bit_use_double_quant,\n",
    "            bnb_4bit_quant_type=params_bnb.bnb_4bit_quant_type\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Trainer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "from transformers import TrainingArguments, TrainerCallback, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "class DebugCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Step: {state.global_step}, Loss: {state.log_history[-1]['loss'] if state.log_history else 'N/A'}\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self,config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.bnb_config = BitsAndBytesConfig(load_in_4bit=self.config.load_in_4bit, bnb_4bit_use_double_quant=self.config.bnb_4bit_use_double_quant, \n",
    "                                             bnb_4bit_quant_type=self.config.bnb_4bit_quant_type, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.config.model_ckpt, quantization_config=self.bnb_config, device_map={\"\":0})\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_ckpt)\n",
    "    \n",
    "    # Dataset Train Test Split\n",
    "    def train_test_split(self,data):\n",
    "\n",
    "        data = data.train_test_split(test_size=0.2)\n",
    "        train_data = data[\"train\"]\n",
    "        test_data = data[\"test\"]\n",
    "\n",
    "        return (train_data,test_data)\n",
    "\n",
    "    # Dataset Creating and Tokenization for Finetuning\n",
    "    def transform_and_tokenize(self):\n",
    "\n",
    "        # Loading the finetuning dataset\n",
    "        finetune_dataframe = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        # Converting the dataframe to dataset\n",
    "        finetune_dataset = Dataset.from_pandas(finetune_dataframe)\n",
    "\n",
    "        # Shuffling and Tokenization\n",
    "        finetune_dataset = finetune_dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "        finetune_dataset = finetune_dataset.map(lambda samples: self.tokenizer(samples[\"prompt\"]), batched=True)\n",
    "\n",
    "        # Train Test Split of Dataset\n",
    "        train_dataset,test_dataset = self.train_test_split(finetune_dataset)\n",
    "\n",
    "        return (train_dataset,test_dataset)\n",
    "    \n",
    "    # Function for preparing the linear layers for training in LoRa\n",
    "    def find_all_linear_names(self,model):\n",
    "        cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "        lora_module_names = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                names = name.split('.')\n",
    "                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "            if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "                lora_module_names.remove('lm_head')\n",
    "        return list(lora_module_names)\n",
    "\n",
    "    # LoRa Configuration for training\n",
    "    def Lora_config(self):\n",
    "\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "        modules = self.find_all_linear_names(self.model)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.r, #  Always keep it 2 times the lora_alpha\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "            target_modules=modules,\n",
    "            lora_dropout=self.config.lora_dropout,\n",
    "            bias=self.config.bias,\n",
    "            task_type=self.config.task_type\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "\n",
    "        return (self.model,lora_config)\n",
    "\n",
    "    def Model_Config(self):\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            warmup_steps = self.config.warmup_steps,\n",
    "            per_device_train_batch_size = self.config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = self.config.gradient_accumulation_steps,\n",
    "            max_steps = self.config.max_steps,\n",
    "            learning_rate = float(self.config.learning_rate),\n",
    "            logging_steps = self.config.logging_steps,\n",
    "            output_dir = self.config.output_dir,\n",
    "            optim = self.config.optim,\n",
    "            save_strategy = self.config.save_strategy\n",
    "        )\n",
    "\n",
    "        return training_args\n",
    "    \n",
    "    def tokenizer_and_model_save(self,model):\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_ckpt,\n",
    "            low_cpu_mem_usage=True,\n",
    "            return_dict=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={\"\": 0},\n",
    "        )\n",
    "        merged_model= PeftModel.from_pretrained(base_model, model)\n",
    "        merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "        # Save the merged model\n",
    "        merged_model.save_pretrained(self.config.model_save_path,safe_serialization=True)\n",
    "        self.tokenizer.save_pretrained(self.config.tokenizer_save_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Loading the tokenized datasets\n",
    "        train_dataset_tokenized , validation_dataset_tokenized = self.transform_and_tokenize()\n",
    "\n",
    "        # Loading the LoRa configured model\n",
    "        model,lora_config = self.Lora_config()\n",
    "\n",
    "        # Setting the Trainer       \n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset_tokenized,\n",
    "            eval_dataset=validation_dataset_tokenized,\n",
    "            dataset_text_field=\"prompt\",\n",
    "            peft_config=lora_config,\n",
    "            args=self.Model_Config(),\n",
    "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),callbacks=[DebugCallback()]\n",
    "        )\n",
    "\n",
    "        # Training the model\n",
    "        model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "        trainer.train()\n",
    "\n",
    "        #Saving the tokenizer and Model\n",
    "        new_model = \"gemma-Exochat-Instruct-Finetune-Step20\"\n",
    "        trainer.model.save_pretrained(new_model)\n",
    "        self.tokenizer_and_model_save(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-16 20:20:28,524: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-06-16 20:20:28,530: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-06-16 20:20:28,533: INFO: common: created directory at: artifacts]\n",
      "[2024-06-16 20:20:28,534: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efe094404f345a5bbdbaf44896f0560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0114b764ee4d0295cb5fbc56a58d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3df1d81d0a34c7d97fbcad5e5164698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff4cf8bcc4451c8c6f03253de095f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87be31465fd548d1bececacb54533a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:555: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: N/A\n",
      "{'loss': 3.4187, 'grad_norm': 2.4991180896759033, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "Step: 2, Loss: 3.4187\n",
      "{'loss': 3.4366, 'grad_norm': 2.4196014404296875, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
      "Step: 3, Loss: 3.4366\n",
      "{'loss': 3.2884, 'grad_norm': 2.343400239944458, 'learning_rate': 6e-05, 'epoch': 0.0}\n",
      "Step: 4, Loss: 3.2884\n",
      "{'loss': 3.0112, 'grad_norm': 1.962088942527771, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
      "Step: 5, Loss: 3.0112\n",
      "{'loss': 2.6853, 'grad_norm': 1.811540961265564, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "Step: 6, Loss: 2.6853\n",
      "{'loss': 2.4543, 'grad_norm': 1.6344647407531738, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
      "Step: 7, Loss: 2.4543\n",
      "{'loss': 2.2487, 'grad_norm': 1.4719736576080322, 'learning_rate': 0.00014, 'epoch': 0.0}\n",
      "Step: 8, Loss: 2.2487\n",
      "{'loss': 1.9105, 'grad_norm': 1.6907520294189453, 'learning_rate': 0.00016, 'epoch': 0.0}\n",
      "Step: 9, Loss: 1.9105\n",
      "{'loss': 1.6332, 'grad_norm': 2.0491721630096436, 'learning_rate': 0.00018, 'epoch': 0.0}\n",
      "Step: 10, Loss: 1.6332\n",
      "{'loss': 1.4581, 'grad_norm': 1.444945216178894, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 1709.6351, 'train_samples_per_second': 0.023, 'train_steps_per_second': 0.006, 'train_loss': 2.5545089483261108, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fe66891e97433eae2ccb998a70e418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.08 GiB is allocated by PyTorch, and 92.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     model_training\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     model_trainer_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_model_trainer_config() \u001b[38;5;66;03m# Storing the configuration\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     model_training \u001b[38;5;241m=\u001b[39m ModelTrainer(config\u001b[38;5;241m=\u001b[39mmodel_trainer_config) \u001b[38;5;66;03m# Using the configuration saved earlier to call model_training\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mmodel_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[13], line 148\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-Exochat-Instruct-Finetune-Step20\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(new_model)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_and_model_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 101\u001b[0m, in \u001b[0;36mModelTrainer.tokenizer_and_model_save\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer_and_model_save\u001b[39m(\u001b[38;5;28mself\u001b[39m,model):\n\u001b[1;32m--> 101\u001b[0m     base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     merged_model\u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, model)\n\u001b[0;32m    109\u001b[0m     merged_model\u001b[38;5;241m=\u001b[39m merged_model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\transformers\\modeling_utils.py:3558\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(dispatch_model)\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m   3557\u001b[0m         device_map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_skip_keys_device_placement\n\u001b[1;32m-> 3558\u001b[0m     dispatch_model(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   3560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3561\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mpostprocess_model(model)\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\accelerate\\big_modeling.py:468\u001b[0m, in \u001b[0;36mdispatch_model\u001b[1;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[0;32m    466\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 468\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\transformers\\modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2555\u001b[0m         )\n\u001b[1;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:849\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 849\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.08 GiB is allocated by PyTorch, and 92.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#Pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config() # Storing the configuration\n",
    "    model_training = ModelTrainer(config=model_trainer_config) # Using the configuration saved earlier to call model_training\n",
    "    model_training.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
