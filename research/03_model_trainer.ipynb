{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects for portfolio\\\\Exoplanet Chatbot\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects for portfolio\\\\Exoplanet Chatbot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig: # defined for the config components present in artifacts for model training\n",
    "    root_dir : Path \n",
    "    data_path : Path\n",
    "    tokenizer_ckpt : Path\n",
    "    model_ckpt : Path\n",
    "    model_save_path: Path\n",
    "    tokenizer_save_path: Path\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    max_steps: int \n",
    "    learning_rate: int\n",
    "    logging_steps: int\n",
    "    output_dir: str\n",
    "    optim: str\n",
    "    save_strategy: str\n",
    "    r: int\n",
    "    lora_alpha: int\n",
    "    lora_dropout: int\n",
    "    bias: int\n",
    "    task_type: int\n",
    "    load_in_4bit: bool\n",
    "    bnb_4bit_use_double_quant: bool\n",
    "    bnb_4bit_quant_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration manager\n",
    "from exoplanet_chatbot.constants import *\n",
    "from exoplanet_chatbot.utils.common import read_yaml,create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH):\n",
    "    # Here we are reading the yaml file and we can now use the file paths and parameter values present inside pararms and config.yaml        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) # Here we are calling the artifacts_root key values using '.' , which was the purpose of @ensure_annotations\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "\n",
    "        config= self.config.model_trainer # Calling the model_trainer dictionary created in config.yaml file\n",
    "        params_training=self.params.TrainingArguments # Calling the TrainingArguments dictionary in params.yaml file\n",
    "        params_lora = self.params.LoraConfig # Calling the Lora Config dictionary in params.yaml file\n",
    "        params_bnb = self.params.BitsandBytesConfig # Calling the BitsandBytesConfig dictionary in params.yaml file\n",
    "\n",
    "        create_directories([config.root_dir]) # Creating a directory using the root directory\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig( # Extracting the values from the config.yaml to here inside data_ingestion_config\n",
    "            #Config parameters\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_ckpt=config.tokenizer_ckpt,\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            model_save_path=config.model_save_path,\n",
    "            tokenizer_save_path=config.tokenizer_save_path,\n",
    "\n",
    "            #Training parameters\n",
    "            warmup_steps=params_training.warmup_steps,\n",
    "            per_device_train_batch_size=params_training.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=params_training.gradient_accumulation_steps,\n",
    "            max_steps=params_training.max_steps,\n",
    "            learning_rate=params_training.learning_rate,\n",
    "            logging_steps=params_training.logging_steps,\n",
    "            output_dir=params_training.output_dir,\n",
    "            optim=params_training.optim,\n",
    "            save_strategy=params_training.save_strategy,\n",
    "\n",
    "            #Lora parameters\n",
    "            r=params_lora.r,\n",
    "            lora_alpha=params_lora.lora_alpha,\n",
    "            lora_dropout=params_lora.lora_dropout,\n",
    "            bias=params_lora.bias,\n",
    "            task_type=params_lora.task_type,\n",
    "\n",
    "            #Bits and bytes Configuration\n",
    "            load_in_4bit=params_bnb.load_in_4bit,\n",
    "            bnb_4bit_use_double_quant=params_bnb.bnb_4bit_use_double_quant,\n",
    "            bnb_4bit_quant_type=params_bnb.bnb_4bit_quant_type\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Trainer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "from transformers import TrainingArguments, TrainerCallback, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "class DebugCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Step: {state.global_step}, Loss: {state.log_history[-1]['loss'] if state.log_history else 'N/A'}\")\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self,config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.bnb_config = BitsAndBytesConfig(load_in_4bit=self.config.load_in_4bit, bnb_4bit_use_double_quant=self.config.bnb_4bit_use_double_quant, \n",
    "                                             bnb_4bit_quant_type=self.config.bnb_4bit_quant_type, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.config.model_ckpt, quantization_config=self.bnb_config, device_map={\"\":0})\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_ckpt)\n",
    "    \n",
    "    # Dataset Train Test Split\n",
    "    def train_test_split(self,data):\n",
    "\n",
    "        data = data.train_test_split(test_size=0.2)\n",
    "        train_data = data[\"train\"]\n",
    "        test_data = data[\"test\"]\n",
    "\n",
    "        return (train_data,test_data)\n",
    "\n",
    "    # Dataset Creating and Tokenization for Finetuning\n",
    "    def transform_and_tokenize(self):\n",
    "\n",
    "        # Loading the finetuning dataset\n",
    "        finetune_dataframe = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        # Converting the dataframe to dataset\n",
    "        finetune_dataset = Dataset.from_pandas(finetune_dataframe)\n",
    "\n",
    "        # Shuffling and Tokenization\n",
    "        finetune_dataset = finetune_dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "        finetune_dataset = finetune_dataset.map(lambda samples: self.tokenizer(samples[\"prompt\"]), batched=True)\n",
    "\n",
    "        # Train Test Split of Dataset\n",
    "        train_dataset,test_dataset = self.train_test_split(finetune_dataset)\n",
    "\n",
    "        return (train_dataset,test_dataset)\n",
    "    \n",
    "    # Function for preparing the linear layers for training in LoRa\n",
    "    def find_all_linear_names(self,model):\n",
    "        cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "        lora_module_names = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                names = name.split('.')\n",
    "                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "            if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "                lora_module_names.remove('lm_head')\n",
    "        return list(lora_module_names)\n",
    "\n",
    "    # LoRa Configuration for training\n",
    "    def Lora_config(self):\n",
    "\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "        modules = self.find_all_linear_names(self.model)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.r, #  Always keep it 2 times the lora_alpha\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "            target_modules=modules,\n",
    "            lora_dropout=self.config.lora_dropout,\n",
    "            bias=self.config.bias,\n",
    "            task_type=self.config.task_type\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "\n",
    "        return (self.model,lora_config)\n",
    "\n",
    "    def Model_Config(self):\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            warmup_steps = self.config.warmup_steps,\n",
    "            per_device_train_batch_size = self.config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = self.config.gradient_accumulation_steps,\n",
    "            max_steps = self.config.max_steps,\n",
    "            learning_rate = self.config.learning_rate,\n",
    "            logging_steps = self.config.logging_steps,\n",
    "            output_dir = self.config.output_dir,\n",
    "            optim = self.config.optim,\n",
    "            save_strategy = self.config.save_strategy\n",
    "        )\n",
    "\n",
    "        return training_args\n",
    "    \n",
    "    def tokenizer_and_model_save(self,model):\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_ckpt,\n",
    "            low_cpu_mem_usage=True,\n",
    "            return_dict=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={\"\": 0},\n",
    "        )\n",
    "        merged_model= PeftModel.from_pretrained(base_model, model)\n",
    "        merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "        # Save the merged model\n",
    "        merged_model.save_pretrained(self.config.model_save_path,safe_serialization=True)\n",
    "        self.tokenizer.save_pretrained(self.config.tokenizer_save_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Loading the tokenized datasets\n",
    "        train_dataset_tokenized , validation_dataset_tokenized = self.transform_and_tokenize()\n",
    "\n",
    "        # Loading the LoRa configured model\n",
    "        model,lora_config = self.Lora_config()\n",
    "\n",
    "        # Setting the Trainer       \n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset_tokenized,\n",
    "            eval_dataset=validation_dataset_tokenized,\n",
    "            dataset_text_field=\"prompt\",\n",
    "            peft_config=lora_config,\n",
    "            args=self.Model_Config(),\n",
    "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),callbacks=[DebugCallback()]\n",
    "        )\n",
    "\n",
    "        # Training the model\n",
    "        model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "        trainer.train()\n",
    "\n",
    "        #Saving the tokenizer and Model\n",
    "        new_model = \"gemma-Exochat-Instruct-Finetune-Step200\"\n",
    "        trainer.model.save_pretrained(new_model)\n",
    "        self.tokenizer_and_model_save(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-16 02:11:49,484: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-06-16 02:11:49,499: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-06-16 02:11:49,503: INFO: common: created directory at: artifacts]\n",
      "[2024-06-16 02:11:49,505: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564e7330e88a4317868bdd94fc0480fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1d0baf692f4e31a70d765faa2e5f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GemmaForCausalLM(\n",
      "      (model): GemmaModel(\n",
      "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-17): 18 x GemmaDecoderLayer(\n",
      "            (self_attn): GemmaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): GemmaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): GemmaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=16384, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=16384, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=16384, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): GELUActivation()\n",
      "            )\n",
      "            (input_layernorm): GemmaRMSNorm()\n",
      "            (post_attention_layernorm): GemmaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): GemmaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config() # Storing the configuration\n",
    "    model_training = ModelTrainer(config=model_trainer_config) # Using the configuration saved earlier to call model_training\n",
    "    model_training.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
